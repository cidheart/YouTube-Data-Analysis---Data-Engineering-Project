{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86248b6b-c303-45ac-9632-72c9e169bd77",
   "metadata": {},
   "source": [
    "Requirement - client is launching a new data-driven campaign on youtube, asks how to categorize videos based on their comments and statistics, and what factors affect how popular a youtube video will be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476aa800-474b-4ba9-b854-49bba5671837",
   "metadata": {},
   "source": [
    "dataset is about top trending videos of different countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af65ba-029d-4b11-8a6f-669b48689f98",
   "metadata": {},
   "source": [
    "create IAM account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26abf6-5fe6-4249-a625-ad49ea590721",
   "metadata": {},
   "source": [
    "we will use aws cli instead of console in this project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b74cd3-430e-42e0-85fc-513f0e6bbd90",
   "metadata": {},
   "source": [
    "and then we connect to our aws account through cmd with credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece007a-e7a6-4d59-9c7a-1adf5a314eea",
   "metadata": {},
   "source": [
    "create s3 bucket through console and upload data through cmd using commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325508d-ff4d-4703-ab0c-cc86dfc5922d",
   "metadata": {},
   "source": [
    "now go to glue and build a crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e098f5-e697-4631-a5c0-d3e0e2b8efba",
   "metadata": {},
   "source": [
    "now query the data using aws athena and create a new bucket for output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9fc1b1-f205-468e-a14c-34800958dcd5",
   "metadata": {},
   "source": [
    "problems - json data cant be readed properly by aws as it identifies only single line key value pair and we have to write etl code for solving this in aws lambda using lambda function (etl code),, we have to increase the time of our lambda code as it will time out if not configured,, package not found error,,while inner joining the two data's of json and csv, the id in json is of string type and that of csv is of int type so we can make the change in the glue tab in edit schema and then we can delete the data and after it we append so it corrected our schema, we should do like this always and avoid cast as it is an expensive process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e6913-9109-419d-873c-aed65ddadf4b",
   "metadata": {},
   "source": [
    "for dealing with the first problem, we have to convert json to parquet format and store it in a different cleansed s3 bucket and then build a glue crawler and query it using athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596bc13-2a87-4ea8-9d34-120fa20e0aa1",
   "metadata": {},
   "source": [
    "lambda layers are added for providing support to packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943f79e-b414-49ac-83e8-1fd5b5af14ed",
   "metadata": {},
   "source": [
    "now the glue catalog has been created by the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249676cb-77ce-4c67-9116-b6d57cb09164",
   "metadata": {},
   "source": [
    "now create a crawler for another datasets which consists of regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87db0f5-8089-45cf-91c0-1b5bacdb17c0",
   "metadata": {},
   "source": [
    "the catalog has been created with partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8753b-7230-4c51-b06d-f7c8862abf20",
   "metadata": {},
   "source": [
    "now for the csv files (raw_statistics folder), we write etl script in glue and store the cleaned data into s3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea6f49-2f94-4048-b9e1-e6e6d234784d",
   "metadata": {},
   "source": [
    "now we can crawl through the cleaned csv to parquet bucket and can query through athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584f3c46-6d0f-42e8-876b-c69458760c71",
   "metadata": {},
   "source": [
    "we can add trigger in lambda also"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384bce09-9771-42b6-871d-708649114e60",
   "metadata": {},
   "source": [
    "now we create a etl pipeline for analytics in aws glue studio which join both cleaned datasets ad a seperate bucket for them as we dont want everytime to do a join operation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a84e25e-63b0-48b7-839c-82e69db66048",
   "metadata": {},
   "source": [
    "and then finally we create dashboard in quicksight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
