{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6d8d2bd4-16e8-44b9-b35b-4c5b7a58ccc0",
   "metadata": {},
   "source": [
    "Let's break down the provided AWS Glue ETL script step by step to understand its functionality.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "This script performs an ETL (Extract, Transform, Load) process using AWS Glue. It reads data from a table in the AWS Glue Data Catalog, applies some transformations, and then writes the transformed data to an S3 bucket in Parquet format. It also uses PySpark for distributed data processing.\n",
    "\n",
    "### Step-by-Step Explanation\n",
    "\n",
    "1. **Importing Necessary Libraries**:\n",
    "   ```python\n",
    "   import sys\n",
    "   from awsglue.transforms import *\n",
    "   from awsglue.utils import getResolvedOptions\n",
    "   from pyspark.context import SparkContext\n",
    "   from awsglue.context import GlueContext\n",
    "   from awsglue.job import Job\n",
    "   from awsglue.dynamicframe import DynamicFrame\n",
    "   ```\n",
    "   - `sys`: Allows interaction with the Python runtime environment.\n",
    "   - `awsglue.transforms`: Contains Glue transformations.\n",
    "   - `awsglue.utils`: Provides utility functions such as `getResolvedOptions`.\n",
    "   - `pyspark.context`: Provides the `SparkContext` needed to initialize Spark.\n",
    "   - `awsglue.context`: Provides the `GlueContext` which is the main entry point for Glue.\n",
    "   - `awsglue.job`: Manages Glue jobs.\n",
    "   - `awsglue.dynamicframe`: Represents a distributed collection of data that can be used in AWS Glue.\n",
    "\n",
    "2. **Parsing Job Arguments**:\n",
    "   ```python\n",
    "   args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "   ```\n",
    "   - This line retrieves the job name passed as an argument when the Glue job is triggered.\n",
    "\n",
    "3. **Initializing Spark and Glue Contexts**:\n",
    "   ```python\n",
    "   sc = SparkContext()\n",
    "   glueContext = GlueContext(sc)\n",
    "   spark = glueContext.spark_session\n",
    "   job = Job(glueContext)\n",
    "   job.init(args['JOB_NAME'], args)\n",
    "   ```\n",
    "   - `SparkContext` is initialized to create a Spark session.\n",
    "   - `GlueContext` is created using the Spark context, enabling integration with AWS Glue.\n",
    "   - The Spark session (`spark`) is extracted from the Glue context.\n",
    "   - A Glue job is created and initialized with the job name.\n",
    "\n",
    "4. **Defining Predicate Pushdown**:\n",
    "   ```python\n",
    "   predicate_pushdown = \"region in ('ca','gb','us')\"\n",
    "   ```\n",
    "   - This predicate filters the data to include only rows where the `region` column value is 'ca', 'gb', or 'us'.\n",
    "\n",
    "5. **Creating a DynamicFrame from the Data Catalog**:\n",
    "   ```python\n",
    "   datasource0 = glueContext.create_dynamic_frame.from_catalog(\n",
    "       database=\"db_youtube_raw\", \n",
    "       table_name=\"raw_statistics\", \n",
    "       transformation_ctx=\"datasource0\", \n",
    "       push_down_predicate=predicate_pushdown\n",
    "   )\n",
    "   ```\n",
    "   - Reads data from the `raw_statistics` table in the `db_youtube_raw` database.\n",
    "   - Applies the `predicate_pushdown` to filter the data.\n",
    "\n",
    "6. **Applying Mapping to Transform Data**:\n",
    "   ```python\n",
    "   applymapping1 = ApplyMapping.apply(\n",
    "       frame=datasource0, \n",
    "       mappings=[\n",
    "           (\"video_id\", \"string\", \"video_id\", \"string\"), \n",
    "           (\"trending_date\", \"string\", \"trending_date\", \"string\"), \n",
    "           (\"title\", \"string\", \"title\", \"string\"), \n",
    "           (\"channel_title\", \"string\", \"channel_title\", \"string\"), \n",
    "           (\"category_id\", \"long\", \"category_id\", \"long\"), \n",
    "           (\"publish_time\", \"string\", \"publish_time\", \"string\"), \n",
    "           (\"tags\", \"string\", \"tags\", \"string\"), \n",
    "           (\"views\", \"long\", \"views\", \"long\"), \n",
    "           (\"likes\", \"long\", \"likes\", \"long\"), \n",
    "           (\"dislikes\", \"long\", \"dislikes\", \"long\"), \n",
    "           (\"comment_count\", \"long\", \"comment_count\", \"long\"), \n",
    "           (\"thumbnail_link\", \"string\", \"thumbnail_link\", \"string\"), \n",
    "           (\"comments_disabled\", \"boolean\", \"comments_disabled\", \"boolean\"), \n",
    "           (\"ratings_disabled\", \"boolean\", \"ratings_disabled\", \"boolean\"), \n",
    "           (\"video_error_or_removed\", \"boolean\", \"video_error_or_removed\", \"boolean\"), \n",
    "           (\"description\", \"string\", \"description\", \"string\"), \n",
    "           (\"region\", \"string\", \"region\", \"string\")\n",
    "       ], \n",
    "       transformation_ctx=\"applymapping1\"\n",
    "   )\n",
    "   ```\n",
    "   - Maps the columns from the source table to the destination format, ensuring the correct data types.\n",
    "\n",
    "7. **Resolving Choices**:\n",
    "   ```python\n",
    "   resolvechoice2 = ResolveChoice.apply(frame=applymapping1, choice=\"make_struct\", transformation_ctx=\"resolvechoice2\")\n",
    "   ```\n",
    "   - Resolves potential ambiguities in the data (e.g., conflicting data types).\n",
    "\n",
    "8. **Dropping Null Fields**:\n",
    "   ```python\n",
    "   dropnullfields3 = DropNullFields.apply(frame=resolvechoice2, transformation_ctx=\"dropnullfields3\")\n",
    "   ```\n",
    "   - Removes any columns or fields that have null values.\n",
    "\n",
    "9. **Converting DynamicFrame to DataFrame and Coalescing Partitions**:\n",
    "   ```python\n",
    "   datasink1 = dropnullfields3.toDF().coalesce(1)\n",
    "   ```\n",
    "   - Converts the DynamicFrame to a Spark DataFrame.\n",
    "   - Uses `coalesce(1)` to reduce the number of partitions to 1, resulting in a single output file.\n",
    "\n",
    "10. **Converting DataFrame Back to DynamicFrame**:\n",
    "    ```python\n",
    "    df_final_output = DynamicFrame.fromDF(datasink1, glueContext, \"df_final_output\")\n",
    "    ```\n",
    "    - Converts the Spark DataFrame back to a DynamicFrame for compatibility with AWS Glue's write methods.\n",
    "\n",
    "11. **Writing the DynamicFrame to S3**:\n",
    "    ```python\n",
    "    datasink4 = glueContext.write_dynamic_frame.from_options(\n",
    "        frame=df_final_output, \n",
    "        connection_type=\"s3\", \n",
    "        connection_options={\"path\": \"s3://de-on-youtube-cleansed-useast1-dev/youtube/raw_statistics/\", \"partitionKeys\": [\"region\"]}, \n",
    "        format=\"parquet\", \n",
    "        transformation_ctx=\"datasink4\"\n",
    "    )\n",
    "    ```\n",
    "    - Writes the transformed DynamicFrame to an S3 bucket in Parquet format.\n",
    "    - Uses `partitionKeys` to partition the data by the `region` column, improving query performance and organization.\n",
    "\n",
    "12. **Committing the Job**:\n",
    "    ```python\n",
    "    job.commit()\n",
    "    ```\n",
    "    - Commits the Glue job, indicating successful completion and triggering any necessary finalization steps.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Initialization**: Set up Spark and Glue contexts, and initialize the Glue job.\n",
    "- **Data Source**: Read data from the AWS Glue Data Catalog with predicate pushdown to filter specific regions.\n",
    "- **Transformation**: Apply column mappings, resolve data type choices, and drop null fields.\n",
    "- **Data Sink**: Convert the DynamicFrame to a DataFrame and back, coalesce partitions, and write the data to an S3 bucket in Parquet format, partitioned by region.\n",
    "- **Job Commit**: Commit the job to complete the ETL process.\n",
    "\n",
    "This script is a typical example of how to use AWS Glue for transforming and moving data in a serverless, scalable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948fe5e-6683-4dd2-9477-1120372ef360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
