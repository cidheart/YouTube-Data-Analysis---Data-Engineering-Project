{
 "cells": [
  {
   "cell_type": "raw",
   "id": "de416d67-3fb0-4f8b-a725-ccd0fb1cf5d0",
   "metadata": {},
   "source": [
    "Sure, let's break down the provided code step by step to understand how it works.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "This AWS Lambda function is designed to be triggered by an S3 event, specifically when an object is created in an S3 bucket. The function reads a JSON file from the S3 bucket, processes it into a Pandas DataFrame, normalizes the JSON data, and then writes the processed data to another S3 location in Parquet format, registering it with AWS Glue Data Catalog.\n",
    "\n",
    "### Step-by-Step Explanation\n",
    "\n",
    "1. **Importing Necessary Libraries**:\n",
    "   ```python\n",
    "   import awswrangler as wr #for accessing files in s3\n",
    "   import pandas as pd\n",
    "   import urllib.parse #manipulating URLs and their components\n",
    "   import os\n",
    "   ```\n",
    "   - `awswrangler` (AWS Data Wrangler) is used for reading and writing data from/to S3 and integrating with AWS services.\n",
    "   - `pandas` is used for data manipulation and analysis.\n",
    "   - `urllib.parse` is used for URL parsing.\n",
    "   - `os` is used for accessing environment variables.\n",
    "\n",
    "2. **Reading Environment Variables**:\n",
    "   ```python\n",
    "   os_input_s3_cleansed_layer = os.environ['s3_cleansed_layer']\n",
    "   os_input_glue_catalog_db_name = os.environ['glue_catalog_db_name']\n",
    "   os_input_glue_catalog_table_name = os.environ['glue_catalog_table_name']\n",
    "   os_input_write_data_operation = os.environ['write_data_operation']\n",
    "   ```\n",
    "   - These lines read configuration values from environment variables. These variables specify the S3 bucket and path for the cleansed data, the Glue Data Catalog database and table names, and the mode for writing data (e.g., 'overwrite', 'append').\n",
    "\n",
    "3. **Lambda Handler Function**:\n",
    "   ```python\n",
    "   def lambda_handler(event, context):\n",
    "   ```\n",
    "   - The `lambda_handler` function is the entry point for the Lambda function. It takes two parameters: `event` and `context`.\n",
    "     - `event`: Contains information about the triggering event (e.g., details about the S3 object that triggered the function).\n",
    "     - `context`: Provides runtime information about the Lambda function execution.\n",
    "\n",
    "4. **Extracting S3 Bucket and Object Key from the Event**:\n",
    "   ```python\n",
    "   bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "   key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')\n",
    "   ```\n",
    "   - These lines extract the S3 bucket name and the object key (file path) from the event data. The object key is URL-decoded using `urllib.parse.unquote_plus`.\n",
    "\n",
    "5. **Try-Except Block for Error Handling**:\n",
    "   ```python\n",
    "   try:\n",
    "   ```\n",
    "   - The `try` block contains the main logic of the function, and the `except` block handles any exceptions that might occur.\n",
    "\n",
    "6. **Reading JSON File from S3 into a DataFrame**:\n",
    "   ```python\n",
    "   df_raw = wr.s3.read_json('s3://{}/{}'.format(bucket, key))\n",
    "   ```\n",
    "   - This line reads the JSON file from the specified S3 bucket and object key into a Pandas DataFrame using AWS Data Wrangler's `read_json` method.\n",
    "\n",
    "7. **Normalizing JSON Data**:\n",
    "   ```python\n",
    "   df_step_1 = pd.json_normalize(df_raw['items'])\n",
    "   ```\n",
    "   - This line normalizes the nested JSON data. The `df_raw` DataFrame is assumed to have an `items` column that contains nested JSON objects. `pd.json_normalize` flattens these nested structures into a flat table.\n",
    "\n",
    "8. **Writing Processed Data to S3 in Parquet Format**:\n",
    "   ```python\n",
    "   wr_response = wr.s3.to_parquet(\n",
    "       df=df_step_1,\n",
    "       path=os_input_s3_cleansed_layer,\n",
    "       dataset=True,\n",
    "       database=os_input_glue_catalog_db_name,\n",
    "       table=os_input_glue_catalog_table_name,\n",
    "       mode=os_input_write_data_operation\n",
    "   )\n",
    "   ```\n",
    "   - This block writes the normalized DataFrame to an S3 bucket in Parquet format. The `path`, `database`, `table`, and `mode` parameters are specified using the environment variables. The `dataset=True` parameter ensures that the data is written in a format compatible with AWS Glue Data Catalog.\n",
    "\n",
    "9. **Returning the Write Response**:\n",
    "   ```python\n",
    "   return wr_response\n",
    "   ```\n",
    "   - This line returns the response from the `to_parquet` function, which includes details about the write operation.\n",
    "\n",
    "10. **Exception Handling**:\n",
    "    ```python\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))\n",
    "        raise e\n",
    "    ```\n",
    "    - If an error occurs during any part of the try block, it is caught by the except block. The error is printed, and a custom error message is also printed. The exception is then re-raised to ensure that the Lambda function reports the error properly.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- The Lambda function is triggered by an S3 event when a new object is created in an S3 bucket.\n",
    "- It reads the JSON file from the S3 bucket into a Pandas DataFrame.\n",
    "- It normalizes the JSON data to flatten nested structures.\n",
    "- It writes the processed data back to another S3 location in Parquet format and registers the data with AWS Glue Data Catalog.\n",
    "- The function handles errors gracefully and logs useful information if an error occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca04b81c-48de-4929-a570-28435a4d8fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
